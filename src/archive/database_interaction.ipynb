{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from src.DataPreprocessing.DataPreprocessing import DataPreprocessing\n",
    "from src.FeatureEngineering.FeatureEngineering import FeatureEngineering\n",
    "from src.PipelineSettings.PipelineSettings import PipelineSettings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all sheets from the .xlsx-file -> See ChatGPT to write this using a for loop and dictionary\n",
    "oil_futures = pd.read_excel('src/data/data/data_PJA.xlsx', decimal=',', parse_dates=True, sheet_name='oil_futures').set_index('Date')\n",
    "gasoline_futures= pd.read_excel('src/data/data/data_PJA.xlsx', decimal=',', parse_dates=True, sheet_name='gasoline_futures').set_index('Date')\n",
    "heating_oil_futures= pd.read_excel('src/data/data/data_PJA.xlsx', decimal=',', parse_dates=True, sheet_name='heating_oil_futures').set_index('Date')\n",
    "oil_futures_new= pd.read_excel('src/data/data/data_PJA.xlsx', decimal=',', parse_dates=True, sheet_name='oil_futures_new').set_index('Date')\n",
    "heating_gasoline_futures= pd.read_excel('src/data/data/data_PJA.xlsx', decimal=',', parse_dates=True, sheet_name='heating_gasoline_futures').set_index('Date')\n",
    "crush_spread_futures= pd.read_excel('src/data/data/data_PJA.xlsx', decimal=',', parse_dates=True, sheet_name='crush_spread_futures').set_index('Date')\n",
    "spark_spread_futures= pd.read_excel('src/data/data/data_PJA.xlsx', decimal=',', parse_dates=True, sheet_name='spark_spread_futures').set_index('Date')\n",
    "diverse_futures= pd.read_excel('src/data/data/data_PJA.xlsx', decimal=',', parse_dates=True, sheet_name='diverse_futures').set_index('Date')\n",
    "governement_bonds_1= pd.read_excel('src/data/data/data_PJA.xlsx', decimal=',', parse_dates=True, sheet_name='governement_bonds_1').set_index('Date')\n",
    "governement_bonds_2= pd.read_excel('src/data/data/data_PJA.xlsx', decimal=',', parse_dates=True, sheet_name='governement_bonds_2').set_index('Date')\n",
    "governement_bonds_3= pd.read_excel('src/data/data/data_PJA.xlsx', decimal=',', parse_dates=True, sheet_name='governement_bonds_3').set_index('Date')\n",
    "metals = pd.read_excel('src/data/data/data_PJA.xlsx', decimal=',', parse_dates=True, sheet_name='metals').set_index('Date')\n",
    "bond_indices = pd.read_excel('src/data/data/data_PJA.xlsx', decimal=',', parse_dates=True, sheet_name='bond_indices').set_index('Date')\n",
    "emmission = pd.read_excel('src/data/data/data_PJA.xlsx', decimal=',', parse_dates=True, sheet_name='emmission').set_index('Date')\n",
    "lumber = pd.read_excel('src/data/data/data_PJA.xlsx', decimal=',', parse_dates=True, sheet_name='Lumber').set_index('Date')\n",
    "coffee = pd.read_excel('src/data/data/data_PJA.xlsx', decimal=',', parse_dates=True, sheet_name='coffee').set_index('Date')\n",
    "corn = pd.read_excel('src/data/data/data_PJA.xlsx', decimal=',', parse_dates=True, sheet_name='corn').set_index('Date')\n",
    "test = pd.read_excel('src/data/data/data_PJA.xlsx', decimal=',', parse_dates=True, sheet_name='test').set_index('Date')\n",
    "gold = pd.read_excel('src/data/data/data_PJA.xlsx', decimal=',', parse_dates=True, sheet_name='gold').set_index('Date')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Merge into one big dataframe\n",
    "dataframes = \\\n",
    "    [\n",
    "    oil_futures, gasoline_futures, heating_oil_futures, oil_futures_new, heating_gasoline_futures,\n",
    "    crush_spread_futures, spark_spread_futures, diverse_futures, governement_bonds_1, metals,\n",
    "    bond_indices, emmission, lumber, coffee, corn, test, governement_bonds_2, governement_bonds_3,\n",
    "    gold\n",
    "    ]\n",
    "data = pd.DataFrame(columns=['Date']).set_index('Date')\n",
    "for frame in dataframes:\n",
    "    data = data.merge(frame, left_index=True, right_index=True, how='outer')\n",
    "    duplicates = data.filter(like='_y', axis=1)\n",
    "    data = data.drop(columns=duplicates.columns)\n",
    "    data.columns = data.columns.str.replace('_x', '')\n",
    "\n",
    "# Perform train-test-split\n",
    "data_train = data[data.index < '2019-01-01']\n",
    "data_test = data[data.index >= '2019-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/PJA/anaconda3/lib/python3.10/site-packages/pandas/core/internals/blocks.py:351: RuntimeWarning: invalid value encountered in log\n",
      "  result = func(self.values, **kwargs)\n",
      "/Users/PJA/anaconda3/lib/python3.10/site-packages/pandas/core/internals/blocks.py:351: RuntimeWarning: invalid value encountered in log\n",
      "  result = func(self.values, **kwargs)\n",
      "/Users/PJA/anaconda3/lib/python3.10/site-packages/pandas/core/internals/blocks.py:351: RuntimeWarning: invalid value encountered in log\n",
      "  result = func(self.values, **kwargs)\n",
      "/Users/PJA/anaconda3/lib/python3.10/site-packages/pandas/core/internals/blocks.py:351: RuntimeWarning: invalid value encountered in log\n",
      "  result = func(self.values, **kwargs)\n",
      "/Users/PJA/anaconda3/lib/python3.10/site-packages/pandas/core/internals/blocks.py:351: RuntimeWarning: invalid value encountered in log\n",
      "  result = func(self.values, **kwargs)\n",
      "/Users/PJA/anaconda3/lib/python3.10/site-packages/pandas/core/internals/blocks.py:351: RuntimeWarning: invalid value encountered in log\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Scaler passed as argument\n"
     ]
    }
   ],
   "source": [
    "# Load Pairs mapping and code-name mapping\n",
    "pairs_mapping = pd.read_excel('src/data/data/codes_mapping.xlsx', sheet_name='mapping')\n",
    "all_pairs_train = pd.DataFrame()\n",
    "for pair in range(0,len(pairs_mapping)):\n",
    "    ps = PipelineSettings().load_settings()\n",
    "    data_ = DataPreprocessing().load_sql(pairs_mapping.loc[pair,'asset_1'], pairs_mapping.loc[pair,'asset_2'])\n",
    "    fs = FeatureEngineering(data_, ps)\n",
    "    features = fs.run_feature_engineering(params_method='Kalman', smoothing=True)\n",
    "    features['pair'] = pair\n",
    "    all_pairs_train = pd.concat([all_pairs_train, features])\n",
    "all_pairs_normalized_train = fs.scale_features(all_pairs_train)[0]\n",
    "all_pairs_test = pd.DataFrame()\n",
    "for pair in range(0,len(pairs_mapping)):\n",
    "    ps = PipelineSettings().load_settings()\n",
    "    data_ = DataPreprocessing().load_sql(pairs_mapping.loc[pair,'asset_1'], pairs_mapping.loc[pair,'asset_2'], table='test_set')\n",
    "    fs = FeatureEngineering(data_, ps)\n",
    "    features = fs.run_feature_engineering(params_method='Kalman', smoothing=True)\n",
    "    features['pair'] = pair\n",
    "    all_pairs_test = pd.concat([all_pairs_test, features])\n",
    "with open('src/Output/scalers/_scaler_2023-06-29_23:10:26.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)   \n",
    "all_pairs_normalized_test = fs.scale_features(all_pairs_train, scaler=scaler)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "all_pairs = pd.DataFrame()\n",
    "for pair in range(0,len(pairs_mapping)):\n",
    "    ps = PipelineSettings().load_settings()\n",
    "    data_ = DataPreprocessing().load_sql(pairs_mapping.loc[pair,'asset_1'], pairs_mapping.loc[pair,'asset_2'])\n",
    "    fs = FeatureEngineering(data_, ps)\n",
    "    features = fs.run_feature_engineering(params_method='Kalman', smoothing=True)\n",
    "    features['pair'] = pair\n",
    "    all_pairs = pd.concat([all_pairs, features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs_normalized_train = fs.scale_features(all_pairs_train)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_pairs \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m pair \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39mlen\u001b[39m(pairs_mapping)):\n\u001b[1;32m      3\u001b[0m     ps \u001b[39m=\u001b[39m PipelineSettings()\u001b[39m.\u001b[39mload_settings()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "all_pairs_test = pd.DataFrame()\n",
    "for pair in range(0,len(pairs_mapping)):\n",
    "    ps = PipelineSettings().load_settings()\n",
    "    data_ = DataPreprocessing().load_sql(pairs_mapping.loc[pair,'asset_1'], pairs_mapping.loc[pair,'asset_2'], table='test_set')\n",
    "    fs = FeatureEngineering(data_, ps)\n",
    "    features = fs.run_feature_engineering(params_method='Kalman', smoothing=True)\n",
    "    features['pair'] = pair\n",
    "    all_pairs_test = pd.concat([all_pairs_test, features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "output_dir = \"../Output/\"\n",
    "time = 'l'\n",
    "with open(f'{output_dir}nets/trained_net_{time}.pkl', 'wb') as f:\n",
    "            pickle.dump(1, f)\n",
    "with open(f'{output_dir}stats/training_stats_{time}.pkl', 'wb') as f:\n",
    "            pickle.dump(2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "conn = sqlite3.connect('src/data/data/pairs_trading.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Store time series in database\n",
    "# data.to_sql('time_series', conn, if_exists='fail', index=True)\n",
    "# data_test.to_sql('test_set', conn, if_exists='fail', index=True)\n",
    "# data_train.to_sql('train_set', conn, if_exists='fail', index=True)\n",
    "# pairs_mapping.to_sql('pairs_mapping', conn, if_exists='fail', index=False)\n",
    "all_pairs_normalized_train.to_sql('training_paires', conn, if_exists='replace', index=False)\n",
    "all_pairs_normalized_test.to_sql('training_paires', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Store pairs mapping for training in database table\n",
    "\n",
    "\n",
    "# Commit the changes to the database and close the connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
